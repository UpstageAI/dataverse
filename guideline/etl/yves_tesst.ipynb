{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import random\n",
    "import binascii\n",
    "from typing import Union, List\n",
    "\n",
    "MERSENNE_PRIME = (1 << 61) - 1\n",
    "MAX_HASH = (1 << 32) - 1\n",
    "HASH_RANGE = 1 << 32\n",
    "\n",
    "def shingle_word(text: str, n_gram: int = 15, char_level: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    example\n",
    "    -------\n",
    "    >>> shingle_word(\"hello world from ducky\", n_gram=2)\n",
    "    ['hello_world', 'world_from', 'from_ducky']\n",
    "\n",
    "    >>> shingle_word(\"hello world from ducky\", n_gram=2, char_level=True)\n",
    "    ['h_e', 'e_l', 'l_l', 'l_o', 'o_w', 'w_o', 'o_r', 'r_l', 'l_d', 'd_f', 'f_r', 'r_o', 'o_m', 'm_d', 'd_u', 'u_c', 'c_k', 'k_y']\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    text_words = text.split() if not char_level else text\n",
    "\n",
    "    for i in range(len(text_words)):\n",
    "        shingle = text_words[i : i + n_gram]\n",
    "\n",
    "        if len(shingle) == n_gram:\n",
    "            res.append(\"_\".join(shingle).encode(\"utf-8\"))\n",
    "\n",
    "    return res\n",
    "\n",
    "def generate_minhash(shingles: List, num_perm: int = 64, seed: int = 1) -> np.array:\n",
    "    def hashfunc(b: bytes) -> bytes:\n",
    "        return binascii.crc32(b) & MAX_HASH\n",
    "\n",
    "    hashvalues = np.ones(num_perm, dtype=np.uint64) * MAX_HASH\n",
    "\n",
    "    generator = np.random.RandomState(seed)\n",
    "    permutations = np.array(\n",
    "        [\n",
    "            (\n",
    "                generator.randint(1, MERSENNE_PRIME, dtype=np.uint64),\n",
    "                generator.randint(0, MERSENNE_PRIME, dtype=np.uint64),\n",
    "            )\n",
    "            for _ in range(num_perm)\n",
    "        ],\n",
    "        dtype=np.uint64,\n",
    "    ).T\n",
    "\n",
    "    for shingle in shingles:\n",
    "        hv = hashfunc(shingle)\n",
    "        a, b = permutations\n",
    "        phv = np.bitwise_and((a * hv + b) % MERSENNE_PRIME, np.uint64(MAX_HASH))\n",
    "        hashvalues = np.minimum(phv, hashvalues)\n",
    "\n",
    "    return hashvalues\n",
    "\n",
    "def expand_instances_by_minhash(\n",
    "    data, expand_size: int, n_gram: int, seed: int = 1, char_level: bool = False\n",
    "):\n",
    "    shingles = shingle_word(data[\"text\"], n_gram=n_gram, char_level=char_level)\n",
    "    minhashes = generate_minhash(shingles, num_perm=expand_size, seed=seed)\n",
    "\n",
    "    for mh in minhashes.tolist():\n",
    "        return (str(mh), [dict(**data, shingles=shingles, hashvalues=minhashes)])\n",
    "\n",
    "\n",
    "def jaccard_by_hashvalues(src_hashvalues, tgt_hashvalues) -> float:\n",
    "    if len(src_hashvalues) != len(tgt_hashvalues):\n",
    "        raise ValueError()\n",
    "\n",
    "    return float(np.count_nonzero(src_hashvalues == tgt_hashvalues)) / float(\n",
    "        len(src_hashvalues))\n",
    "\n",
    "\n",
    "def explore_dedup_instance(hash_groups, threshold: float = 0.8):\n",
    "    if len(hash_groups) <= 1:\n",
    "        return\n",
    "\n",
    "    group_represent_text = hash_groups[0][\n",
    "        \"text\"\n",
    "    ]  # not to remove all text instances in group.\n",
    "    pairs = combinations(hash_groups, 2)\n",
    "\n",
    "    for d_1, d_2 in pairs:\n",
    "        sim_score = jaccard_by_hashvalues(d_1[\"hashvalues\"], d_2[\"hashvalues\"])\n",
    "        if sim_score >= threshold:\n",
    "            dedup_text = [d_1[\"text\"], d_2[\"text\"]]\n",
    "            if group_represent_text in dedup_text:\n",
    "                dedup_text[0] if dedup_text[\n",
    "                    0\n",
    "                ] != group_represent_text else dedup_text[1]\n",
    "            else:\n",
    "                random.choice(dedup_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [\n",
    "        {\"text\": \"hello wolrd! Welcome to dataverse.\"},\n",
    "        {\"text\": \"hello wolrd! Welcome to dataverrrse.\"},\n",
    "        {\"text\": \"a totally different sentence\"},\n",
    "    ]\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_instances_by_minhash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataverse/lib/python3.11/site-packages/pandas/core/series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataverse/lib/python3.11/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataverse/lib/python3.11/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/dataverse/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataverse/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_expanded \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mexpand_instances_by_minhash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[18], line 61\u001b[0m, in \u001b[0;36mexpand_instances_by_minhash\u001b[0;34m(data, expand_size, n_gram, seed, char_level)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpand_instances_by_minhash\u001b[39m(\n\u001b[1;32m     59\u001b[0m     data, expand_size: \u001b[38;5;28mint\u001b[39m, n_gram: \u001b[38;5;28mint\u001b[39m, seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, char_level: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     60\u001b[0m ):\n\u001b[0;32m---> 61\u001b[0m     shingles \u001b[38;5;241m=\u001b[39m shingle_word(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, n_gram\u001b[38;5;241m=\u001b[39mn_gram, char_level\u001b[38;5;241m=\u001b[39mchar_level)\n\u001b[1;32m     62\u001b[0m     minhashes \u001b[38;5;241m=\u001b[39m generate_minhash(shingles, num_perm\u001b[38;5;241m=\u001b[39mexpand_size, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mh \u001b[38;5;129;01min\u001b[39;00m minhashes\u001b[38;5;241m.\u001b[39mtolist():\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "df_expanded = data[\"text\"].apply(lambda x : expand_instances_by_minhash(x, 64, 2, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_minhash = expand_instances_by_minhash(data.to_dict(orient=\"series\"), 64, 2, char_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('423976591',\n",
       " [{'text': 0      hello wolrd! Welcome to dataverse.\n",
       "   1    hello wolrd! Welcome to dataverrrse.\n",
       "   2            a totally different sentence\n",
       "   Name: text, dtype: object,\n",
       "   'shingles': [b'hello wolrd! Welcome to dataverse._hello wolrd! Welcome to dataverrrse.',\n",
       "    b'hello wolrd! Welcome to dataverrrse._a totally different sentence'],\n",
       "   'hashvalues': array([ 423976591, 1179597160, 2173486595, 2656929283, 1905198105,\n",
       "           466826231, 2483130910, 1749155483, 1906894582,  574160600,\n",
       "           291037418, 1661721150,  471728005, 2009185705, 2294101579,\n",
       "           492540358, 1166238982, 1402358624,  773040593,  831153555,\n",
       "          1510210255, 2078701579, 2744031734, 1490008704, 3654404923,\n",
       "           358977674,  940225680,  191411290,  358731490,   97368304,\n",
       "           209788358, 1994545281,  207854605, 1308159746, 3207868941,\n",
       "          2745819470, 2539655204, 2657916589, 1785459166,   97784208,\n",
       "          1344271224, 2487440659, 3090907716,  815100042,  781975625,\n",
       "           204015332,  595921339, 1848543554, 1455026746, 3053947088,\n",
       "           467188972,  193648503, 2205537551,  795729082,   68253029,\n",
       "           133506038, 1853197176,  171252027,  995871745,  244694855,\n",
       "           367552121,  308558434, 2219776208, 1997736408], dtype=uint64)}])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles, hashvalues = [], []\n",
    "for row in data.iterrows():\n",
    "    v = row[1]\n",
    "    _, tmp = expand_instances_by_minhash(v, 64,2)\n",
    "    # print(expand_instances_by_minhash(v, 64,2))\n",
    "    shingles.append(tmp[0][\"shingles\"])\n",
    "    hashvalues.append(tmp[0][\"hashvalues\"])\n",
    "data[\"shingles\"] = shingles\n",
    "data[\"hashvalues\"] = hashvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'hello wolrd! Welcome to dataverse.',\n",
       "  'shingles': [b'hello_wolrd!',\n",
       "   b'wolrd!_Welcome',\n",
       "   b'Welcome_to',\n",
       "   b'to_dataverse.'],\n",
       "  'hashvalues': array([1125711376, 1585414850,  113619648, 1095903752, 1007095010,\n",
       "          349694736,   61628889, 1561244383, 1107734654,  582302568,\n",
       "          962267556,   86318050,  684024235,  500219653, 2378927720,\n",
       "          193711265,  471341851, 2201276130,  368868197,  532525545,\n",
       "         3102877401,   55324660, 1447957220, 2403186128,  180222207,\n",
       "           75847955,  332420352, 1096705033,  865950441,   77055466,\n",
       "          379314978,  319043347,   50939237,  494688010, 2029015502,\n",
       "          123625184, 1933699250,  323107670,  109296489,  106212960,\n",
       "          220407054,  972873360,  521857045,    4794227, 1421693921,\n",
       "          774197356, 1173623403, 2020963681,  756702417,   79241917,\n",
       "          334425944, 2816280587,   53941169,  557843763,    4611238,\n",
       "         1849989506,  638516427,   80694179, 1884461840, 1485031525,\n",
       "           13681054, 2105429315, 1338547253,  995240497], dtype=uint64)},\n",
       " {'text': 'hello wolrd! Welcome to dataverrrrse.',\n",
       "  'shingles': [b'hello_wolrd!',\n",
       "   b'wolrd!_Welcome',\n",
       "   b'Welcome_to',\n",
       "   b'to_dataverrrrse.'],\n",
       "  'hashvalues': array([1125711376, 1585414850,  113619648,  536451411, 1007095010,\n",
       "          349694736,   61628889, 1561244383, 1107734654, 2564804360,\n",
       "          962267556,  903880077,  684024235,  112344574, 2009773094,\n",
       "          193711265,  471341851,  331765307,  368868197,  532525545,\n",
       "         1527841847,   11860759,  350719676, 2403186128,  555922535,\n",
       "          340427954, 1196656551, 1096705033,  865950441,   77055466,\n",
       "          379314978, 1720063000,   50939237,  494688010,  224368933,\n",
       "          215850680, 1663696025,  323107670,  109296489,  106212960,\n",
       "          220407054,  972873360,  212556956,    4794227, 1421693921,\n",
       "          774197356, 1173623403, 2020963681, 1974219351,   79241917,\n",
       "          334425944, 2816280587,   53941169,  557843763,    4611238,\n",
       "         1849989506,  729960856,  261252883, 1884461840, 1485031525,\n",
       "           13681054, 2105429315, 1338547253,  917020894], dtype=uint64)},\n",
       " {'text': 'a totally different sentence',\n",
       "  'shingles': [b'a_totally', b'totally_different', b'different_sentence'],\n",
       "  'hashvalues': array([1258860144,  667928298, 1762760336, 2634665171, 2509363010,\n",
       "          338504404,  757136128, 2689306977,   24046950,  465313421,\n",
       "         1909008427,  744473789,  766018655, 3018215877,   68726391,\n",
       "         2239407416,  531255206,   40130475,  694049904, 1025755889,\n",
       "         1394677387, 2032100500,  118844233, 1791269629, 2514770848,\n",
       "          431416470, 2181903800,   24237628, 1589577519, 2041005352,\n",
       "         1593655091,  703708051, 2872989689,   86900824,  134909797,\n",
       "         1283698268,   89429771,  107850210, 1185549196,  798001211,\n",
       "          336832449, 3573690456,  124309120,   37596544, 2671576336,\n",
       "         2704691935,  858476412,  591685805, 2094595693,  185548520,\n",
       "          166699455,   57381210,   89877669,  815116652,  778372917,\n",
       "         1286700603, 2897181443,  777504942,  710456283, 1065007797,\n",
       "         1892797537, 1894127355, 2665966826, 1708264395], dtype=uint64)}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = explore_dedup_instance(data.to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mexplore_dedup_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[13], line 49\u001b[0m, in \u001b[0;36mexplore_dedup_instance\u001b[0;34m(hash_groups, threshold)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hash_groups) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m group_represent_text \u001b[38;5;241m=\u001b[39m \u001b[43mhash_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# not to remove all text instances in group.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m pairs \u001b[38;5;241m=\u001b[39m combinations(hash_groups, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d_1, d_2 \u001b[38;5;129;01min\u001b[39;00m pairs:\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "print(explore_dedup_instance(data[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from dataverse.etl import ETLPipeline\n",
    "from dataverse.etl import register_etl\n",
    "from dataverse.etl import ETLRegistry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================\n",
       "Total [ 45 ]\n",
       "==================================================\n",
       "data_ingestion [ 17 ]\n",
       "deduplication [ 4 ]\n",
       "cleaning [ 14 ]\n",
       "pii [ 2 ]\n",
       "quality [ 1 ]\n",
       "data_load [ 4 ]\n",
       "utils [ 3 ]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ETLRegistry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_etl\n",
    "def data_ingestion___test___generate_accent(spark, *args, **kwargs):\n",
    "    data = [(\"café\",), (\"résumé\",), (\"piñata\",)]\n",
    "    df = spark.createDataFrame(data, [\"text\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/dataverse/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (InvalidClientTokenId) when calling the GetCallerIdentity operation: The security token included in the request is invalid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dataverse.etl.pipeline.ETLPipeline at 0x7f36a28bd150>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl_pipeline = ETLPipeline()\n",
    "etl_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ No AWS Credentials Found] - Failed to set spark conf for S3\n"
     ]
    }
   ],
   "source": [
    "spark, data = etl_pipeline.sample(sample_etl=\"data_ingestion___test___generate_accent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://instance-4113:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[10]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f35f03197d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "# load from dict\n",
    "ETL_config = OmegaConf.create({\n",
    "    'spark': {\n",
    "        'appname': 'ETL',\n",
    "        'driver': {'memory': '16g'},\n",
    "    },\n",
    "    'etl': [\n",
    "        {\n",
    "            'name': 'data_ingestion___test___generate_accent',\n",
    "        },\n",
    "        {'name': 'cleaning___accent___remove'}\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark:\n",
      "  appname: ETL\n",
      "  driver:\n",
      "    memory: 16g\n",
      "etl:\n",
      "- name: data_ingestion___test___generate_accent\n",
      "- name: cleaning___accent___remove\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(ETL_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ No AWS Credentials Found] - Failed to set spark conf for S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/17 05:54:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark, data = etl_pipeline.run(ETL_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyspark.sql.session.SparkSession at 0x7f35f03197d0>,\n",
       " PythonRDD[21] at RDD at PythonRDD.scala:53)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'cafe'}, {'text': 'resume'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'The key [ cleaning___accent___remove ] is already registered'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataverse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01metl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcleaning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cleaning__accent___remove\n",
      "File \u001b[0;32m~/anaconda3/envs/dataverse/lib/python3.11/site-packages/dataverse/etl/cleaning/accent.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;129;43m@register_etl\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mcleaning___accent___remove\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRDD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;43;03m    strip accents from a piece of text\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;43;03m    - example\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;43;03m        subset (str): subset or columns to consider\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataverse/lib/python3.11/site-packages/dataverse/etl/registry.py:328\u001b[0m, in \u001b[0;36mregister_etl\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    324\u001b[0m etl_file_path \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetfile(func)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# I know using class name without snake case is awkward\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# but I want to keep the class name as it is and user won't know it\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m etl_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    329\u001b[0m     func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    330\u001b[0m     (BaseETL,),\n\u001b[1;32m    331\u001b[0m     {\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m: add_self(func),\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file_path__\u001b[39m\u001b[38;5;124m\"\u001b[39m: etl_file_path,\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__etl_dir__\u001b[39m\u001b[38;5;124m\"\u001b[39m: etl_file_path\u001b[38;5;241m.\u001b[39mstartswith(ETL_DIR),\n\u001b[1;32m    335\u001b[0m     }\n\u001b[1;32m    336\u001b[0m )\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m etl_cls\n",
      "File \u001b[0;32m~/anaconda3/envs/dataverse/lib/python3.11/site-packages/dataverse/etl/registry.py:279\u001b[0m, in \u001b[0;36mETLAutoRegistry.__new__\u001b[0;34m(cls, name, bases, attrs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirect inheritance from BaseETL not allowed. Use @register_etl decorator.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     registry \u001b[38;5;241m=\u001b[39m ETLRegistry()\n\u001b[0;32m--> 279\u001b[0m     \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_class\n",
      "File \u001b[0;32m~/anaconda3/envs/dataverse/lib/python3.11/site-packages/dataverse/etl/registry.py:134\u001b[0m, in \u001b[0;36mETLRegistry.register\u001b[0;34m(self, key, etl)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# register already exists\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe key [ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ] is already registered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry[key] \u001b[38;5;241m=\u001b[39m etl\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_status(key\u001b[38;5;241m=\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'The key [ cleaning___accent___remove ] is already registered'"
     ]
    }
   ],
   "source": [
    "from dataverse.etl.cleaning.accent import cleaning__accent___remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
